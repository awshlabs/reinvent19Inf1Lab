# Lab 2. Deploy and Serve Compiled Model On Inferentia Instance (30 Minutes)

**NOTE: Please complete Lab 1 before starting Lab 2 to obtain the compiled model. **

## Lab 2 Section 1: Select Deep Learning AMI Version 26 for Ubuntu, and start an Inf1 instance inf1.2xlarge.

2.1 Select Deep Learning AMI Version 26 for Ubuntu, and start an Inf1 instance inf1.2xlarge. 

## Lab 2 Section 2: Update Repo List for Pip and Apt-get (Debian)

2.2.1 Add Neuron Apt-get repo for debian, Ubuntu 16.
>For Ubuntu 18, use bionic instead of xenial; we are using Ubuntu 16 for this lab.

Adding Repo URL to list.
```bash
sudo tee /etc/apt/sources.list.d/neuron.list > /dev/null <<EOF
deb https://apt.repos.neuron.amazonaws.com xenial main
EOF
```
Add apt-get public key for the Neuron Apt-get Repo.
```bash
wget -qO - https://apt.repos.neuron.amazonaws.com/GPG-PUB-KEY-AMAZON-AWS-NEURON.PUB | sudo apt-key add -
```

2.2.2 Installing Neuron run-time, SDK, and Tensorflow Model server optimized for the Neuron SDK.

```bash
sudo apt-get update
```
```bash
sudo apt-get install aws-neuron-runtime
```
```bash
sudo apt-get install aws-neuron-tools
```
Tensorflow model server that can take neuron parameter files and serve it directly.
```bash
sudo apt-get install tensorflow-model-server-neuron
```

2.2.3. Add Neuron Pip Repo to system for easy update.

```bash
sudo tee /etc/pip.conf > /dev/null <<EOF
[global]
extra-index-url = https://pip.repos.neuron.amazonaws.com
EOF
```

2.2.4. Install virtual environment:

```bash
sudo apt-get update
sudo apt-get -y install virtualenv
```

2.2.5. Setup a new Python 3.6 environment:

```bash
virtualenv --python=python3.6 test_env_p36
```

```bash
source test_env_p36/bin/activate
```

2.2.6. Install TensorFlow-Neuron (Neuron Compiler is not needed if compilation is not done)
```bash
pip install tensorflow-neuron
```

2.2.7. Also, you would need TensorFlow Serving API (use --no-deps to prevent installation of regular tensorflow):

```bash
pip install --no-deps tensorflow_serving_api==1.15
```

2.2.8. Install Pillow (graphics package for loading images)
```bash
pip install pillow
```

### Lab 2 Section 3: On the Inf1, create a inference Python script named infer_resnet50.py with the following content:

Steps Overview:
2.3.1. You already compiled the Keras ResNet50 model and export it as a SavedModel which is an interchange format for TensorFlow models. ssh copy (scp) the compiled package onto the Inferentia Instance.
2.3.2 to 2.3.5. Run inference on Inf1 with an example input.


2.3.1. If not compiling and inferring on the same instance, copy the artifact to the inference server:

On your C5 instance create a .pem file with your private key; you can again use nano or vi to copy and paste your ee-default-keypair.pem from your local laptop.

Now, copy the compiled tf model package to your Inf1 instance:

```bash
scp -i ee-default-keypair.pem ./resnet50_neuron.zip ubuntu@<instance DNS>:~/ # Ubuntu Image default.
#scp -i ee-default-keypair.pem ./resnet50_neuron.zip ec2-user@<instance DNS>:~/  # if on AML2  if you are on Amazon Linux Image.
```
2.3.2. On the Inf1, create a inference Python script named `infer_resnet50.py` with the following content:
```python
import os
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import resnet50

tf.keras.backend.set_image_data_format('channels_last')

# Create input from image
img_sgl = image.load_img('kitten_small.jpg', target_size=(224, 224))
img_arr = image.img_to_array(img_sgl)
img_arr2 = np.expand_dims(img_arr, axis=0)
img_arr3 = resnet50.preprocess_input(img_arr2)

# Load model
COMPILED_MODEL_DIR = './resnet50_neuron/'
predictor_inferentia = tf.contrib.predictor.from_saved_model(COMPILED_MODEL_DIR)

# Run Inference and Display results
model_feed_dict={'input': img_arr3}
infa_rslts = predictor_inferentia(model_feed_dict)
print(resnet50.decode_predictions(infa_rslts["output"], top=5)[0])
```

2.3.3. Unzip the mode on Inferentia Instance, download the example image and run the inference:
```bash
unzip resnet50_neuron.zip
```
2.3.4. Get a sample image to test the model inference on Neuron Cores.
```bash
curl -O https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg
```
2.3.5. Run Inference Script
```bash
python infer_resnet50.py
```
You should get:
```bash
[('n02123045', 'tabby', 0.69945353), ('n02127052', 'lynx', 0.1215847), ('n02123159', 'tiger_cat', 0.08367486), ('n02124075', 'Egyptian_cat', 0.064890705), ('n02128757', 'snow_leopard', 0.009392076)]
```

## Lab 2 Section 4.  Neuron TensorFlow Serving

TensorFlow Serving is a serving system that allows customers to scale-up inference across a network. Neuron TensorFlow Serving uses the same API as normal TensorFlow Serving. The only differences are that the saved model must be compiled for Inferentia and the entry point is a different binary named `tensorflow_model_server_neuron`. The binary is found at `/usr/local/bin/tensorflow_model_server_neuron` and is pre-installed in the DLAMI or installed with APT/YUM tensorflow-model-server-neuron package.

You have installed these two packages in the previous step.

The following example shows how to prepare saved model for serving and a sample inference via served model.

2.4.1. Prepare Compiled Saved Model

Prepare a directory structure for TensorFlow model serving, with the previously compiled ResNet50 saved model in directory "1":

```bash
mkdir -p resnet50_inf1_serve
cp -rf resnet50_neuron resnet50_inf1_serve/1
```

2.4.2. Serving Saved Model

User can now serve the saved model with the tensorflow_model_server_neuron binary:

```bash
tensorflow_model_server_neuron --model_name=resnet50_inf1_serve --model_base_path=$(pwd)/resnet50_inf1_serve/ --port=8500
```

The compiled model is staged in Inferentia DRAM by the server to prepare for inference.

2.4.3. Generate inference requests to the model server

In another terminal, enter the created virtualenv:

```bash
source test_env_p36/bin/activate
```

Run inferences via GRPC using the following sample client code (save it as `tfs_client.py` and run it as `python tfs_client.py`):

```python
import numpy as np
import grpc
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.applications.resnet50 import decode_predictions
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc

tf.keras.backend.set_image_data_format('channels_last')

if __name__ == '__main__':
    channel = grpc.insecure_channel('localhost:8500')
    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
    img_file = tf.keras.utils.get_file(
        "./kitten_small.jpg",
        "https://raw.githubusercontent.com/awslabs/mxnet-model-server/master/docs/images/kitten_small.jpg")
    img = image.load_img(img_file, target_size=(224, 224))
    img_array = preprocess_input(image.img_to_array(img)[None, ...])
    request = predict_pb2.PredictRequest()
    request.model_spec.name = 'resnet50_inf1_serve'
    request.inputs['input'].CopyFrom(
        tf.contrib.util.make_tensor_proto(img_array, shape=img_array.shape))
    result = stub.Predict(request)
    prediction = tf.make_ndarray(result.outputs['output'])
    print(decode_predictions(prediction))
```

Expected output:
```bash
[[('n02123045', 'tabby', 0.69945353), ('n02127052', 'lynx', 0.1215847), ('n02123159', 'tiger_cat', 0.08367486), ('n02124075', 'Egyptian_cat', 0.064890705), ('n02128757', 'snow_leopard', 0.009392076)]]
```

2.4.4 Cleanup

Please terminate the TensorFlow Serving process in the first terminal by pressing Ctrl-C and do: 

```bash
/opt/aws/neuron/bin/neuron-cli reset
```
