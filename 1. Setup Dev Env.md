# Reinvent Inf1 Lab: Hands-on Deep Learning Inference with Amazon EC2 Inf1 Instance (20 min)

## Abstract:

In this workshop, you gain hands-on experience with Amazon EC2 Inf1 instances, powered by custom AWS Inferentia chips. Amazon EC2 Inf1 instances offer low-latency, high-throughput, and cost-effective machine learning inference in the cloud. This workshop walks you through taking a trained deep learning model to deployment on Amazon EC2 Inf1 instances by using AWS Neuron, an SDK for optimizing inference using AWS Inferentia processors.


## Overview:

**Note: Please folllow the labs in sequence.**

Lab 1. **Launch** a C5 Instance, **install** the Neuron development environment, Custom compile a pre-trained model to target the Inferentia Neuron Processor.

Lab 2. **Launch** an Inf1 Instance, **install** Neuron run-time and development environment, **test** and **model serve** the compiled ResNet package.

Lab 3. **Compile and Launch** a load test on Inferentia Instance.

Lab 4. **Debug and Profile** your model.


----------

# Lab 1. **Launch** a C5 Instance, **install** the Neuron development environment, Custom compile a pre-trained model to target the Inferentia Neuron Processor.

## Lab 1 Section 1: Launch an EC2 instance to setup Neuron SDK Dev Environment

A typical workflow with the Neuron SDK will be to compile a trained ML model on a compute-optimized compilation server and then the distribute the artifacts to Inf1 instances for execution.  Select an AMI of your choice, which may be Ubuntu 16.x, Ubuntu 18.x, Amazon Linux 2 based. To use a pre-built Deep Learning AMI, which includes all of the needed packages, see these instructions: https://docs.aws.amazon.com/dlami/latest/devguide/launch-config.html.

**1.1.0** Setup your SSH env.  For Windows, we recommend installing MobaXterm. https://mobaxterm.mobatek.net/

If you are using AWS Events engine onsite at Re:Invent. Download your SSH key from the events engine dashboard.  https://dashboard.eventengine.run/ Enter your unique events hash, and download the SSH Key to your local laptop/machine.


**1.1.1** Goto the EC2 portal, select, **Deep Learning AMI (Ubuntu 16.04) Version 26.**  We will run through the installation and update process as an exercise, as we are very actively updating the software tools and packages.
>Please update your system frequently to ensure you are using the latest Neuron packages.

**1.1.2** Select and start an EC2 instance as your development environment.
It is recommended to use c5.4xlarge or larger. For this example we will use a **c5d.4xlarge** with a faster SSD drive, 16 vcores, and 32 GB of RAM.

>In the future, if you would like to compile and run inference on the same machine, please select inf1.6xlarge or larger.


**1.1.3** Choose an existing ssh key.


**1.1.4** Install virtual environment:
> It is always a best practice to use a virtual environment to ensure you have the best flexibility for package management in working with deep learning frameworks.
```bash
sudo apt-get update
sudo apt-get -y install virtualenv
```

Note: If you see the following errors during apt-get install, please wait a minute or so for background updates to finish and retry apt-get install:

```bash
E: Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?
```

**1.1.5** Setup a new Python 3.6 virtual environment.

```bash
virtualenv --python=python3.6 test_env_p36
source test_env_p36/bin/activate
pip install --no-deps tensorflow_serving_api==1.15
```
> If you are ever disconnected from the instance, make sure you run this command again to get back to the correct virtual environment where you have installed all the correct packages.


```bash
source test_env_p36/bin/activate
```

**1.1.6** Modify Pip configurations to point to the Neuron repository.
```bash
tee $VIRTUAL_ENV/pip.conf > /dev/null <<EOF
[global]
extra-index-url = https://pip.repos.neuron.amazonaws.com
EOF
```


**1.1.7** Install the Neuron Compiler software and TensorFlow package with Neuron support.

```bash
pip install neuron-cc tensorflow-neuron
```
Please ignore the following error displayed during installation:
```bash
ERROR: tensorflow-serving-api 1.15.0 requires tensorflow~=1.15.0, which is not installed.
```

**1.1.8** You may keep your session alive by configuring your ~/.ssh/config
```bash
echo "ClientAliveInterval 60" >> ~/.ssh/config
```

## Lab #1 Section 2.  Compile model for deployment onto Inf1 Instance.

### Steps Overview:

**1.2.1** Compile the Keras ResNet50 model and export it as a SavedModel which is an interchange format for TensorFlow models.
**1.2.2** Run inference on Inf1 with an example input.

>Note that Model size for Resnet is about 20+ million parameters, in FP32 format, each parameter is 4 bytes.  The compiler process automatically converts them into BF16 (2 bytes per parameter), which is a much more efficient dataformat that Neuron Cores support in hardware.

**1.2.3** Create a python script named `compile_resnet50.py` with the following content. You can use either nano or vi editors.

>Inspect the code below to understand the steps: export savedModel, compile model.

```python
import os
import time
import shutil
import tensorflow as tf
import tensorflow.neuron as tfn
import tensorflow.compat.v1.keras as keras
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input

# Create a workspace
WORKSPACE = './ws_resnet50'
os.makedirs(WORKSPACE, exist_ok=True)

# Prepare export directory (old one removed)
model_dir = os.path.join(WORKSPACE, 'resnet50')
compiled_model_dir = os.path.join(WORKSPACE, 'resnet50_neuron')
shutil.rmtree(model_dir, ignore_errors=True)
shutil.rmtree(compiled_model_dir, ignore_errors=True)

# Instantiate Keras ResNet50 model
keras.backend.set_learning_phase(0)
tf.keras.backend.set_image_data_format('channels_last')
model = ResNet50(weights='imagenet')

# Export SavedModel
tf.saved_model.simple_save(
    session            = keras.backend.get_session(),
    export_dir         = model_dir,
    inputs             = {'input': model.inputs[0]},
    outputs            = {'output': model.outputs[0]})

# Compile using Neuron
#tfn.saved_model.compile(model_dir, compiled_model_dir) #default compiles to 1 neuron core.
tfn.saved_model.compile(model_dir, compiled_model_dir, compiler_args =['--num-neuroncores', '4']) # compile to 4 neuron cores.

# Prepare SavedModel for uploading to Inf1 instance
shutil.make_archive('./resnet50_neuron', 'zip', WORKSPACE, 'resnet50_neuron')
```

> Note `compiler_args =['--num-neuroncores', '4']`, targets and optimizes the model to run on 4 Neuron cores; default is 1. Be sure to use the correct number of cores on the target Inf1 instance.


**1.2.4** Run the compilation script, which will take a few minutes on c5d.4xlarge. At the end of script execution, the compiled SavedModel is zipped as resnet50_neuron.zip in local directory. This zip file will be needed in the next lab where we deploy the compiled model on an Inf1 instance.

```bash
time python compile_resnet50.py  
```
```
...
INFO:tensorflow:fusing subgraph neuron_op_d6f098c01c780733 with neuron-cc
INFO:tensorflow:Number of operations in TensorFlow session: 4638
INFO:tensorflow:Number of operations after tf.neuron optimizations: 556
INFO:tensorflow:Number of operations placed on Neuron runtime: 554
INFO:tensorflow:Successfully converted ./ws_resnet50/resnet50 to ./ws_resnet50/resnet50_neuron

real    1m54.833s
user    1m46.876s
sys     0m6.736s
```

>Note: Neuron compiler does ahead-time compilation. Comparing this to JIT, or first time inference compile systems, this saves time if you are deploying this onto mutliple servers. The Neuron compiler can automatically fuse operator + scheduling and mem management.

**1.2.5** Please keep resnet50_neuron.zip in local directory. You will need this in the next lab. Please go to the next document to launch an Inf1 Instance to deploy your compiled model.
